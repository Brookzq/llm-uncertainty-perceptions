This is the official code of the paper titled _Perceptions of Linguistic Uncertainty by Language Models and Humans_ (2024).

**Abstract**: Uncertainty expressions such as _probably_ or _highly unlikely_ are pervasive in human language. While prior work has established that there is general population-level agreement among humans about what these expressions mean quantitatively, the abilities of LLMs to interpret these phrases have seen little investigation. In this paper, we __introduce a task for evaluating the abilities of LLMs to interpret uncertainty expressions as probabilities__. Our approach assesses whether LLMs can employ _theory of mind_ in this setting: understanding the uncertainty of another agent about a particular statement, independently of the LLM's own certainty about that statement. We evaluate both humans and popular LLMs on this task and find that __current LLMs are suprisingly able to map uncertainty expressions__ to probabilistic responses in a human-like manner. However, __we observe systematically different behavior depending on whether a statement is actually true or false__. This sensitivity indicates that LLMs are substantially more susceptible to bias based on their prior knowledge (as compared to humans). These findings raise crucial questions and have broad implications for human-AI and AI-AI communication of uncertainty.

## Dataset 

Coming soon...

## Code

Coming soon...

