{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5e710a7-9026-43b2-a8ee-6352ed82da69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "\n",
    "import glob\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def get_llm_correctness_results(results_fp: str, ground_truth_fp: str, single_prompt_substr: str | None=None):\n",
    "    def remove_puctuation(text):\n",
    "        import string\n",
    "        return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    def majority_voting(df):\n",
    "        counts = df.value_counts()\n",
    "        majority = sorted(counts.items(), key=lambda x: x[1])[-1]\n",
    "        conf = majority[1] / len(df)\n",
    "        return  majority[0], conf\n",
    "\n",
    "    answer_per_type = {\n",
    "        \"non-verifiable\": \"unknown\",\n",
    "        \"verifiable-true\": \"true\",\n",
    "        \"verifiable-false\": \"false\",\n",
    "    }\n",
    "    \n",
    "    ground_truth = pd.read_csv(ground_truth_fp)\n",
    "    ground_truth = ground_truth.drop_duplicates(\"prompt\", keep=\"first\")\n",
    "    ground_truth[\"correct_response\"] = ground_truth[\"type\"].apply(lambda x: answer_per_type[x])\n",
    "    \n",
    "    results = pd.read_csv(results_fp).drop(\"ix\", axis=1)\n",
    "    # Filter the results by the prompts that contain a specific substring\n",
    "    if single_prompt_substr is not None:\n",
    "        results = results[results[\"prompt\"].apply(lambda p: single_prompt_substr in p)]\n",
    "\n",
    "    \n",
    "    results[\"__orig_pred_response\"] = results[\"responses\"]\n",
    "    results[\"model\"] = results_fp.rpartition(\"/\")[-1].rpartition(\".csv\")[0]\n",
    "    results[\"responses\"] = results[\"responses\"].apply(str.lower).apply(str.strip)\n",
    "    results[\"responses\"] = results[\"responses\"].apply(remove_puctuation)\n",
    "    results = results.rename({\"responses\": \"pred_response\"}, axis=1)\n",
    "    results = ground_truth.set_index(\"prompt\").join(results.set_index(\"prompt\"), how=\"right\")\n",
    "\n",
    "    results[\"pred_response__mode\"] = None\n",
    "    results[\"pred_response_conf__mode\"] = None\n",
    "    # Determine the final prediction and its' confidence (using self-consistency approach)\n",
    "    for statement in results[\"statement\"].unique():\n",
    "        st_subset = results[results[\"statement\"] == statement]\n",
    "        resp, freq = majority_voting(st_subset[\"pred_response\"])\n",
    "        results.loc[results[\"statement\"] == statement, \"pred_response__mode\"] = resp\n",
    "        results.loc[results[\"statement\"] == statement, \"pred_response_conf__mode\"] = freq\n",
    "        \n",
    "    # Select only 1 row per example\n",
    "    results = results.groupby(\"statement\").head(1)\n",
    "    results[\"accuracy\"] = results[\"correct_response\"] == results[\"pred_response__mode\"]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea266e30-fa5a-419b-beef-f6b34a76f85f",
   "metadata": {},
   "source": [
    "In this section, we analyse the extent to which LLMs belief of correctness are related to the ground truth correctness of the statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7149fb2-7ba0-4e6c-a4e1-b6e87ec68997",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"../../results/correctness-vs-llm-belief\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2eeb05-892e-460c-a62e-ca988a6790b9",
   "metadata": {},
   "source": [
    "## 1. Main Experiment (single prompt only: True, False, Unknown)\n",
    "\n",
    "We obtained results for 4 different models using three different prompts. The three different prompts aim to marginalize over potential biases that the models may have in predicting one of the labels `True`, `False`, `Unknown`. \n",
    "\n",
    "The evaluated models are: \n",
    "- `gpt-3.5-turbo-0125`\n",
    "- `gpt-4-turbo-2024-04-09`\n",
    "- `gpt-4o-2024-05-13`\n",
    "- `gemini-pro`\n",
    "\n",
    "Each prompt is executed 7 times using the configurations: \n",
    "```python\n",
    "max_tokens = 30\n",
    "temperature = 0.5\n",
    "n_samples = 7\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d64ba6a7-cabb-48d6-aa4f-cb8fed4bd2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../results/outputs/correctness-vs-llm-belief/main_exp_true_false3/gpt-3.5-turbo-0125.csv\n",
      "110\n",
      "30\n",
      "30\n",
      "../../results/outputs/correctness-vs-llm-belief/main_exp_true_false3/gpt-4-turbo-2024-04-09.csv\n",
      "110\n",
      "30\n",
      "30\n",
      "../../results/outputs/correctness-vs-llm-belief/main_exp_true_false3/gpt-4o-2024-05-13.csv\n",
      "110\n",
      "30\n",
      "30\n",
      "../../results/outputs/correctness-vs-llm-belief/main_exp_true_false3/models/gemini-pro.csv\n",
      "110\n",
      "30\n",
      "30\n",
      "\\begin{tabular}{lrrrr}\n",
      "\\toprule\n",
      " & avg acc & non-verifiable & verifiable-true & verifiable-false \\\\\n",
      "model &  &  &  &  \\\\\n",
      "\\midrule\n",
      "gpt-3.5-turbo-0125 & 90.000000 & 90.000000 & 90.000000 & 90.000000 \\\\\n",
      "gpt-4-turbo-2024-04-09 & 98.820000 & 100.000000 & 100.000000 & 93.330000 \\\\\n",
      "gpt-4o-2024-05-13 & 98.820000 & 99.090000 & 100.000000 & 96.670000 \\\\\n",
      "gemini-pro & 90.000000 & 90.910000 & 93.330000 & 83.330000 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = defaultdict(list)\n",
    "\n",
    "ground_truth_fp = \"../../data/assumptions/main-exp_true_false3.csv\"\n",
    "main_exp_filepaths = glob.glob(\"../../results/outputs/correctness-vs-llm-belief/main_exp_true_false3/**/*.csv\", recursive=True)\n",
    "for fp in main_exp_filepaths:\n",
    "    print(fp)\n",
    "    df = get_llm_correctness_results(fp, ground_truth_fp, single_prompt_substr=\"True, False, Unknown\")\n",
    "    results[\"model\"].append(df[\"model\"].values[0])\n",
    "    results[\"avg acc\"].append(np.round((df[\"accuracy\"].mean() * 100), 2))\n",
    "\n",
    "    for typ in (\"non-verifiable\", \"verifiable-true\", \"verifiable-false\"):\n",
    "        subset_typ = df[df[\"type\"] == typ]\n",
    "        print(len(subset_typ))\n",
    "        results[typ].append(np.round((subset_typ[\"accuracy\"].mean() * 100), 2))\n",
    "print(pd.DataFrame(results).set_index(\"model\").to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d2beab-1374-478a-b1ef-ccaa9f3cf442",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
